import torch
from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler,TensorDataset
from torch.utils.data.distributed import DistributedSampler
from transformers import (WEIGHTS_NAME, AdamW, get_linear_schedule_with_warmup,
						  RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer)
import os
import sys
import time
import numpy as np
import pandas as pd

import hashlib
from collections import Counter
import subprocess
import pickle
from linevul.TSParse import TSParse

import pandas as pd
import numpy as np
import json
import random
from tqdm import tqdm
import imblearn
from imblearn.under_sampling import RandomUnderSampler
from imblearn.over_sampling import RandomOverSampler

from git import Commit, Repo
from pydriller import Repository, ModificationType, Git as PyDrillerGitRepo

import cleanlab
from cleanlab.classification import CleanLearning
from cleanlab.filter import find_label_issues

from linevul_model import Model

import tensorflow as tf
from scipy.spatial import distance


tf.random.set_seed(2)

os.environ['PYTHONHASHSEED']= '42'

########################################################################################

def makedir(path):
	if not os.path.exists(path):
		print("Creating", path)
		os.makedirs(path)

########################################################################################
class InputFeatures(object):
	"""A single training/test features for a example."""
	def __init__(self,
				 input_tokens,
				 input_ids,
				 label):
		self.input_tokens = input_tokens
		self.input_ids = input_ids
		self.label=label
		

class TextDataset(Dataset):
	def __init__(self, tokenizer, args, inputs, labels):
		
		self.examples = []
		
		# print(inputs)
		# print(labels)
		
		for i in tqdm(range(len(inputs))):
			self.examples.append(convert_examples_to_features(inputs[i], labels[i], tokenizer, args))
		
	def __len__(self):
		return len(self.examples)

	def __getitem__(self, i):		
		return torch.tensor(self.examples[i].input_ids),torch.tensor(self.examples[i].label)


def convert_examples_to_features(func, label, tokenizer, args):
	
	# source
	code_tokens = tokenizer.tokenize(str(func))[:args.block_size-2]
	source_tokens = [tokenizer.cls_token] + code_tokens + [tokenizer.sep_token]
	source_ids = tokenizer.convert_tokens_to_ids(source_tokens)
	padding_length = args.block_size - len(source_ids)
	source_ids += [tokenizer.pad_token_id] * padding_length
	
	return InputFeatures(source_tokens, source_ids, label)


def set_seed(args):
	random.seed(args.seed)
	np.random.seed(args.seed)
	torch.manual_seed(args.seed)
	if args.n_gpu > 0:
		torch.cuda.manual_seed_all(args.seed)


#####
def evaluate_transformer(args, model, tokenizer, eval_dataset, eval_when_training=False):
	#build dataloader
	eval_sampler = SequentialSampler(eval_dataset)
	eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler,batch_size=args.eval_batch_size,num_workers=0)

	# multi-gpu evaluate
	if args.n_gpu > 1 and eval_when_training is False:
		model = torch.nn.DataParallel(model)

	# Eval!
	print("***** Running evaluation *****")
	print("	 Num examples = %d", len(eval_dataset))
	print("	 Batch size = %d", args.eval_batch_size)
	
	eval_loss = 0.0
	nb_eval_steps = 0
	model.eval()
	logits=[]  
	y_trues=[]
	for batch in eval_dataloader:
		(inputs_ids, labels)=[x.to(args.device) for x in batch]
		with torch.no_grad():
			lm_loss, logit = model(input_ids=inputs_ids, labels=labels)
			eval_loss += lm_loss.mean().item()
			logits.append(logit.cpu().numpy())
			y_trues.append(labels.cpu().numpy())
		nb_eval_steps += 1
	
	#calculate scores
	logits = np.concatenate(logits,0)
	y_trues = np.concatenate(y_trues,0)
	best_threshold = 0.5
	best_f1 = 0
	y_preds = logits[:,1]>best_threshold
	acc = accuracy_score(y_trues, y_preds)
	recall = recall_score(y_trues, y_preds)
	precision = precision_score(y_trues, y_preds)	
	f1 = f1_score(y_trues, y_preds)
	result = {
		"eval_accuracy": float(acc),
		"eval_recall": float(recall),
		"eval_precision": float(precision),
		"eval_f1": float(f1),		
		"eval_threshold":best_threshold,
	}

	print("***** Eval results *****")
	for key in sorted(result.keys()):
		print("	 %s = %s", key, str(round(result[key],4)))

	return result


class dotdict(dict):
	"""dot.notation access to dictionary attributes"""
	__getattr__ = dict.get
	__setattr__ = dict.__setitem__
	__delattr__ = dict.__delitem__


class Transformer_Model:
	def __init__(self, x_train, y_train, x_val, y_val, model_folder, best_model_name):
	
		self.args = {}
		self.args["output_dir"] = model_folder
		self.args["model_type"] = "roberta"
		self.args["block_size"] = 512
		self.args["model_name"] = best_model_name
		self.args["model_name_or_path"] = 'microsoft/codebert-base'
		self.args["config_name"] = ""
		self.args["tokenizer_name"] = 'microsoft/codebert-base'
		self.args["code_length"] = 256
		self.args["train_batch_size"] = 32
		self.args["eval_batch_size"] = 32
		self.args['gradient_accumulation_steps'] = 1
		self.args["learning_rate"] = 1e-5
		self.args["weight_decay"] = 0.0
		self.args["adam_epsilon"] = 1e-8
		self.args["max_grad_norm"] = 1.0
		self.args["max_steps"] = -1
		self.args["warmup_steps"] = 0
		self.args['seed'] = 42
		self.args['epochs'] = 10
		self.args['num_attention_heads'] = 12
		self.args['n_gpu'] = torch.cuda.device_count()
		self.args['device'] = torch.device("cuda" if torch.cuda.is_available() else "cpu")
		
		self.args = dotdict(self.args)
		
		# Set seed
		set_seed(self.args)
		config = RobertaConfig.from_pretrained(self.args.config_name if self.args.config_name else self.args.model_name_or_path)
		config.num_labels = 1
		config.num_attention_heads = self.args.num_attention_heads
		
		logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',datefmt='%m/%d/%Y %H:%M:%S',level=logging.INFO)
		print("device: %s, n_gpu: %s", self.args.device, self.args.n_gpu)
		
		self.tokenizer = RobertaTokenizer.from_pretrained(self.args.tokenizer_name)
		
		self.model = RobertaForSequenceClassification.from_pretrained(self.args.model_name_or_path, config=config, ignore_mismatched_sizes=True)	
		
		self.model = Model(self.model, config, self.tokenizer, self.args)
		print("Training/evaluation parameters %s", self.args)
		
		self.train_dataset = TextDataset(self.tokenizer, self.args, x_train, y_train)
		self.eval_dataset = TextDataset(self.tokenizer, self.args, x_val, y_val)
		
	
	def fit(self,x,y):
		""" Train the model """
		# build dataloader
		train_sampler = RandomSampler(self.train_dataset)
		train_dataloader = DataLoader(self.train_dataset, sampler=train_sampler, batch_size=self.args.train_batch_size, num_workers=0)
		
		self.args.max_steps = self.args.epochs * len(train_dataloader)
		# evaluate the model per epoch
		self.args.save_steps = len(train_dataloader)
		self.args.warmup_steps = self.args.max_steps // 5
		self.model.to(self.args.device)

		# Prepare optimizer and schedule (linear warmup and decay)
		no_decay = ['bias', 'LayerNorm.weight']
		optimizer_grouped_parameters = [
			{'params': [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],
			 'weight_decay': self.args.weight_decay},
			{'params': [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}
		]
		optimizer = AdamW(optimizer_grouped_parameters, lr=self.args.learning_rate, eps=self.args.adam_epsilon)
		scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=self.args.warmup_steps,
													num_training_steps=self.args.max_steps)

		# multi-gpu training
		if self.args.n_gpu > 1:
			self.model = torch.nn.DataParallel(self.model)

		# Train!
		print("***** Running training *****")
		print("	 Num examples = %d", len(self.train_dataset))
		print("	 Num Epochs = %d", self.args.epochs)
		print("	 Instantaneous batch size per GPU = %d", self.args.train_batch_size//max(self.args.n_gpu, 1))
		print("	 Total train batch size = %d",self.args.train_batch_size*self.args.gradient_accumulation_steps)
		print("	 Gradient Accumulation steps = %d", self.args.gradient_accumulation_steps)
		print("	 Total optimization steps = %d", self.args.max_steps)
		
		global_step=0
		tr_loss, logging_loss, avg_loss, tr_nb, tr_num, train_loss = 0.0, 0.0, 0.0, 0, 0, 0
		best_f1=-0.1 # To save model with f1=0, rare but may happen

		self.model.zero_grad()

		for idx in range(self.args.epochs): 
			bar = tqdm(train_dataloader,total=len(train_dataloader))
			tr_num = 0
			train_loss = 0
			for step, batch in enumerate(bar):
				(inputs_ids, labels) = [x.to(self.args.device) for x in batch]
				self.model.train()
				loss, logits = self.model(input_ids=inputs_ids, labels=labels)
				if self.args.n_gpu > 1:
					loss = loss.mean()
				if self.args.gradient_accumulation_steps > 1:
					loss = loss / self.args.gradient_accumulation_steps

				loss.backward()
				torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.args.max_grad_norm)

				tr_loss += loss.item()
				tr_num += 1
				train_loss += loss.item()
				if avg_loss == 0:
					avg_loss = tr_loss
					
				avg_loss = round(train_loss/tr_num,5)
				bar.set_description("epoch {} loss {}".format(idx,avg_loss))
				  
				if (step + 1) % self.args.gradient_accumulation_steps == 0:
					optimizer.step()
					optimizer.zero_grad()
					scheduler.step()  
					global_step += 1
					output_flag=True
					avg_loss=round(np.exp((tr_loss - logging_loss) /(global_step- tr_nb)),4)

					if global_step % self.args.save_steps == 0:
						results = evaluate_transformer(self.args, self.model, self.tokenizer, self.eval_dataset, eval_when_training=True)	 
						
						# Save model checkpoint
						if results['eval_f1']>best_f1:
							best_f1=results['eval_f1']
							print("	 "+"*"*20)	
							print("	 Best f1:%s",round(best_f1,4))
							print("	 Respective accuracy:%s",round(results['eval_accuracy'],4))
							print("	 Respective precision:%s",round(results['eval_precision'],4))
							print("	 Respective recall:%s",round(results['eval_recall'],4))
							print("	 "+"*"*20)						  
							
							output_dir = self.args.output_dir
							
							if not os.path.exists(output_dir):
								os.makedirs(output_dir)						   
							model_to_save = self.model.module if hasattr(self.model,'module') else self.model
							output_dir = os.path.join(output_dir, '{}'.format(self.args.model_name)) 
							torch.save(model_to_save.state_dict(), output_dir)
							print("Saving model checkpoint to %s", output_dir)


	def perform_predict(self,x):
		# build dataloader
		y = np.asarray([0] * len(x))
		test_dataset = TextDataset(self.tokenizer, self.args, x, y)
		
		test_sampler = SequentialSampler(test_dataset)
		test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=self.args.eval_batch_size, num_workers=0)
		
		# multi-gpu evaluate
		if self.args.n_gpu > 1:
			self.model = torch.nn.DataParallel(self.model)

		# Eval!
		print("***** Running Test *****")
		print("	 Num examples = %d", len(test_dataset))
		print("	 Batch size = %d", self.args.eval_batch_size)
		
		best_threshold = 0.5
		nb_eval_steps = 0
		self.model.eval()
		logits=[]  
		
		for batch in test_dataloader:
			(inputs_ids, labels) = [x.to(self.args.device) for x in batch]
			with torch.no_grad():
				lm_loss, logit = self.model(input_ids=inputs_ids, labels=labels)
				logits.append(logit.cpu().numpy())
			nb_eval_steps += 1
		
		# calculate scores
		logits = np.concatenate(logits, 0)
		y_pred = logits[:, 1] > best_threshold
		
		return y_pred.astype(int)


#####
def predict_proba(x, model, tokenizer, args):

	# print(args)
	# build dataloader
	y = np.asarray([0] * len(x))
	test_dataset = TextDataset(tokenizer, args, x, y)
	
	test_sampler = SequentialSampler(test_dataset)
	test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=args.eval_batch_size, num_workers=0)

	# multi-gpu evaluate
	if args.n_gpu > 1:
		model = torch.nn.DataParallel(model)

	# Eval!
	print("***** Running Test *****")
	print("	 Num examples = %d", len(test_dataset))
	print("	 Batch size = %d", args.eval_batch_size)
	
	best_threshold = 0.5
	nb_eval_steps = 0
	model.eval()
	logits=[]  
	
	for batch in test_dataloader:
		(inputs_ids, labels) = [x.to(args.device) for x in batch]
		# print(inputs_ids, labels)
		with torch.no_grad():
			lm_loss, logit = model(input_ids=inputs_ids, labels=labels)
			# print(logit.cpu().numpy())
			logits.append(logit.cpu().numpy())
		nb_eval_steps += 1
	
	# calculate scores
	# print(logits)
	logits = np.concatenate(logits, 0)

	return logits
	
	
def perform_predict(x, model, tokenizer, args):

	y = np.asarray([0] * len(x))
	test_dataset = TextDataset(tokenizer, args, x, y)
	
	test_sampler = SequentialSampler(test_dataset)
	test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=args.eval_batch_size, num_workers=0)

	# multi-gpu evaluate
	if args.n_gpu > 1:
		model = torch.nn.DataParallel(model)

	# Eval!
	print("***** Running Test *****")
	print("	 Num examples = %d", len(test_dataset))
	print("	 Batch size = %d", args.eval_batch_size)
	
	best_threshold = 0.5
	nb_eval_steps = 0
	model.eval()
	logits=[]  
	
	for batch in test_dataloader:
		(inputs_ids, labels) = [x.to(args.device) for x in batch]
		# print(inputs_ids, labels)
		with torch.no_grad():
			lm_loss, logit = model(input_ids=inputs_ids, labels=labels)
			# print(logit.cpu().numpy())
			logits.append(logit.cpu().numpy())
		nb_eval_steps += 1
	
	# calculate scores
	# print(logits)
	logits = np.concatenate(logits, 0)
	y_pred = logits[:, 1] > best_threshold
		
	return y_pred.astype(int)


def extract_features(model, x, args):

	y = np.asarray([0] * len(x))
	test_dataset = TextDataset(tokenizer, args, x, y)
	
	test_sampler = SequentialSampler(test_dataset)
	test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=args.eval_batch_size, num_workers=0)

	# multi-gpu evaluate
	if args.n_gpu > 1:
		model = torch.nn.DataParallel(model)

	cls_features = []

	for batch in test_dataloader:
		(inputs_ids, labels) = [x.to(args.device) for x in batch]
		with torch.no_grad():
			if len(inputs_ids) == 1:
				cls_features.extend([model(input_ids=inputs_ids, output_cls=True).squeeze().detach().cpu().numpy().tolist()])
			else:
				cls_features.extend(model(input_ids=inputs_ids, output_cls=True).squeeze().detach().cpu().numpy().tolist())

	print(len(cls_features), len(cls_features[0]))
	return cls_features


def generate_transformer_model(model_folder, best_model_name):
	
	args = {}
	args["output_dir"] = model_folder
	args["model_type"] = "roberta"
	args["block_size"] = 512
	args["model_name"] = best_model_name
	args["model_name_or_path"] = 'microsoft/codebert-base'
	args["config_name"] = ""
	args["tokenizer_name"] = 'microsoft/codebert-base'
	args["code_length"] = 256
	args["train_batch_size"] = 32
	args["eval_batch_size"] = 32
	args['gradient_accumulation_steps'] = 1
	args["learning_rate"] = 1e-5
	args["weight_decay"] = 0.0
	args["adam_epsilon"] = 1e-8
	args["max_grad_norm"] = 1.0
	args["max_steps"] = -1
	args["warmup_steps"] = 0
	args['seed'] = 42
	args['epochs'] = 10
	args['num_attention_heads'] = 12
	args['n_gpu'] = torch.cuda.device_count()
	args['device'] = torch.device("cuda" if torch.cuda.is_available() else "cpu")
	
	args = dotdict(args)

	# Set seed
	set_seed(args)
	config = RobertaConfig.from_pretrained(args.config_name if args.config_name else args.model_name_or_path)
	config.num_labels = 1
	config.num_attention_heads = args.num_attention_heads
	
	tokenizer = RobertaTokenizer.from_pretrained(args.tokenizer_name)
	
	model = RobertaForSequenceClassification.from_pretrained(args.model_name_or_path, config=config, ignore_mismatched_sizes=True)	
	
	model = Model(model, config, tokenizer, args)

	return model, tokenizer, args


def convert_str_to_int(s):
	result = int(''.join(list(str(ord(character)) for character in s))) % 123456
	return result


model_folder = 'linevul/saved_models/checkpoint-best-f1/'
data_folder = 'data_devign'
project_name = 'all'
setting = 'my_non_vfc'


def get_cmd_clean(config, model_seed, data_seed, model_name, train_file, eval_file, test_file, out_file, function=False, test_only=False, tp_indices_file=''):
	
	if test_only:
		cmd = f"""python linevul/linevul_main.py \
		  --output_dir=linevul/saved_models \
		  --model_type=roberta \
		  --model_name={model_name} \
		  --tokenizer_name=microsoft/codebert-base \
		  --model_name_or_path=microsoft/codebert-base \
		  --do_test \
		  --write_results=True \
		  --train_data_file={train_file} \
		  --eval_data_file={eval_file} \
		  --test_data_file={test_file} \
		  --epochs 10 \
		  --block_size 512 \
		  --train_batch_size 32 \
		  --eval_batch_size 32 \
		  --learning_rate 1e-5 \
		  --max_grad_norm 1.0 \
		  --evaluate_during_training \
		  --seed {model_seed}  2>&1 | tee {out_file}
		"""
		print(cmd, flush=True)
		return cmd
	
	if function:
		cmd = f"""python linevul/linevul_main.py \
		  --output_dir=linevul/saved_models \
		  --model_type=roberta \
		  --model_name={model_name} \
		  --tokenizer_name=microsoft/codebert-base \
		  --model_name_or_path=microsoft/codebert-base \
		  --do_train \
		  --do_test \
		  --write_results=True \
		  --train_data_file={train_file} \
		  --eval_data_file={eval_file} \
		  --test_data_file={test_file} \
		  --epochs 10 \
		  --block_size 512 \
		  --train_batch_size 32 \
		  --eval_batch_size 32 \
		  --learning_rate 1e-5 \
		  --max_grad_norm 1.0 \
		  --evaluate_during_training \
		  --seed {model_seed}  2>&1 | tee {out_file}
		"""
	else:
		cmd = f"""python linevul/linevul_main.py \
		  --output_dir=linevul/saved_models \
		  --model_type=roberta \
		  --model_name={model_name} \
		  --tokenizer_name=microsoft/codebert-base \
		  --model_name_or_path=microsoft/codebert-base \
		  --do_test \
		  --do_local_explanation \
		  --top_k_constant=10 \
		  --do_sorting_by_line_scores \
		  --effort_at_top_k=0.2 \
		  --top_k_recall_by_lines=0.01 \
		  --top_k_recall_by_pred_prob=0.2 \
		  --reasoning_method=attention \
		  --load_results=True \
		  --tp_indices_file={tp_indices_file} \
		  --train_data_file={train_file} \
		  --eval_data_file={eval_file} \
		  --test_data_file={test_file} \
		  --epochs 10 \
		  --block_size 512 \
		  --train_batch_size 32 \
		  --eval_batch_size 32 \
		  --learning_rate 1e-5 \
		  --max_grad_norm 1.0 \
		  --evaluate_during_training \
		  --seed {model_seed}  2>&1 | tee {out_file}
		"""
	print(cmd, flush=True)
	return cmd


start_time = time.time()

n_repeats = 10
metrics = {}
configs = [sys.argv[1]]

for config in configs:
	metrics[config] = {'F1': [], 'Recall': [], 'Precision': [],
					   'Top-10_Accuracy': [], 'IFA': [], 'Effort@0.2Recall': [], 'Recall@0.01LOC': []}

for i in range(n_repeats):
	print('#' * 20)
	print(f'Repeat {i + 1}')
	
	print('#' * 20, '\n')
	print('Function-level evaluation starts here!!!')
	
	data_seed = (i + 1) * 10000
	
	for config in configs:
		print('#' * 20, '\n')
		print('Config:', config)
		print('Training the model')

		train_data_file = f'{data_folder}/train_{config}_{project_name}_{setting}_{data_seed}_all.parquet'
		eval_data_file = f'{data_folder}/val_{project_name}_{setting}_{data_seed}_all.parquet'
		test_data_file = f'{data_folder}/test_{project_name}_{setting}_{data_seed}_all.parquet'

		train_df = pd.read_parquet(train_data_file)
		
		print('Size of original train_df:', len(train_df), flush=True)

		print('Extracting non-latent and latent data', flush=True)
		train_non_latent_df = train_df[~(train_df['latent'] == 1)]
		
		train_latent_df = train_df[train_df['latent'] == 1]
		x_train_latent = train_latent_df['code'].values
		print('Len of train_latent_df:', len(train_latent_df), flush=True)

		train_non_latent_file = f'{data_folder}/train_{config}_{project_name}_{setting}_{data_seed}_predict_non_latent.parquet'
		train_latent_file = f'{data_folder}/train_{config}_{project_name}_{setting}_{data_seed}_predict_latent.parquet'

		train_non_latent_df.to_parquet(train_non_latent_file, index=False)
		train_latent_df.to_parquet(train_latent_file, index=False)

		print('Extracting temp val non-vulnerable file for training', flush=True)
		val_df = pd.read_parquet(eval_data_file)
		val_df_tmp = val_df.loc[val_df['label'] == 0].copy()
		x_val_tmp = val_df_tmp['code'].values
		y_val_tmp = val_df_tmp['label'].values
		print('Len of val_df_tmp:', len(val_df_tmp), flush=True)

		train_latent_df_tmp = train_latent_df.copy()
		
		model_seed = convert_str_to_int(f'{config}_{project_name}_{setting}') + (i + 1) * 20000

		# Train with all train non-latent data first
		best_model_name = f'model_{config}_{project_name}_{setting}_{data_seed}_predict_non_latent.bin'
		out_file = f'linevul/train_logs_devign/train_{config}_{project_name}_{setting}_{data_seed}_predict_func_non_latent.log'
		
		with subprocess.Popen(get_cmd_clean(config, model_seed, data_seed, best_model_name, train_non_latent_file, eval_data_file, test_data_file, out_file, function=True), cwd=None,
						  shell=True, stdout=subprocess.PIPE) as proc:
			output = [x.decode("utf-8") for x in proc.stdout.readlines()]

		clf, tokenizer, args = generate_transformer_model(model_folder, best_model_name)

		device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
		clf.load_state_dict(torch.load(f'{model_folder}{best_model_name}', map_location=device))
		clf.to(device)

		print('Removing intermediate model', flush=True)
		os.remove(f'{model_folder}{best_model_name}')

		clean_start_time = time.time()
		
		y_latent_pred = perform_predict(x_train_latent, clf, tokenizer, args)
		print('Prediction proportion:', Counter(y_latent_pred))
		
		# Find the indices of the cases where the model predicts as vulnerable in the latent data
		pos_indices = np.where((y_latent_pred == True) | (y_latent_pred == 1))[0]

		train_latent_df_tmp.reset_index(drop=True, inplace=True)
		train_latent_df_tmp = train_latent_df_tmp.loc[~train_latent_df_tmp.index.isin(pos_indices)].reset_index(drop=True)

		print('Cleaning time:', time.time() - clean_start_time, 's.', flush=True)
		
		if len(train_latent_df_tmp) == 0:
			print('No labels found at', threshold, flush=True)
			continue
		
		print('Size of cleaned latent:', len(train_latent_df_tmp), flush=True)

		train_df_new = pd.concat([train_non_latent_df, train_latent_df_tmp], ignore_index=True)
		train_data_file_new = f'{data_folder}/train_{config}_{project_name}_{setting}_{data_seed}_all_predict.parquet'
		train_df_new.to_parquet(train_data_file_new, index=False)

		best_model_name = f'model_{config}_{project_name}_{setting}_{data_seed}_predict_clean.bin'
		out_file = f'linevul/train_logs_devign/train_{config}_{project_name}_{setting}_{data_seed}_predict_func_clean.log'
		
		print('Starting training with cleaned latent using self-prediction', flush=True)

		with subprocess.Popen(get_cmd_clean(config, model_seed, data_seed, best_model_name, train_data_file_new, eval_data_file, test_data_file, out_file, function=True), cwd=None,
						  shell=True, stdout=subprocess.PIPE) as proc:
			output = [x.decode("utf-8") for x in proc.stdout.readlines()]
			for result in output:
				if 'test_recall' in result:
					print(f"Recall: {float(result.split(' = ')[1].rstrip())}")
					metrics[config]['Recall'].append(float(result.split(' = ')[1].rstrip()))
				elif 'test_precision' in result:
					print(f"Precision: {float(result.split(' = ')[1].rstrip())}")
					metrics[config]['Precision'].append(float(result.split(' = ')[1].rstrip()))
				elif 'test_f1' in result:
					print(f"F1: {float(result.split(' = ')[1].rstrip())}")
					metrics[config]['F1'].append(float(result.split(' = ')[1].rstrip()))

		for metric in metrics[config]:
			if len(metrics[config][metric]) > 0:
				print(metric, np.mean(metrics[config][metric]), np.std(metrics[config][metric]))

		print('Removing latent training files', flush=True)
		# Clean up intermediate files
		os.remove(train_non_latent_file)
		os.remove(train_latent_file)
		
	tp_indices_file = ''
	
	print('#' * 20, '\n')
	print('Line-level evaluation starts here!!!')
	for config in configs:
		print('#' * 20, '\n')
		print('Config:', config)

		model_seed = convert_str_to_int(f'{config}_{project_name}_{setting}') + (i + 1) * 20000
		
		train_data_file_new = f'{data_folder}/train_{config}_{project_name}_{setting}_{data_seed}_all_predict.parquet'
		eval_data_file = f'{data_folder}/val_{project_name}_{setting}_{data_seed}_all.parquet'
		test_data_file = f'{data_folder}/test_{project_name}_{setting}_{data_seed}_all.parquet'

		best_model_name = f'model_{config}_{project_name}_{setting}_{data_seed}_predict_clean.bin'
		out_file = f'linevul/train_logs_devign/train_{config}_{project_name}_{setting}_{data_seed}_predict_line_clean.log'

		with subprocess.Popen(get_cmd_clean(config, model_seed, data_seed, best_model_name, train_data_file_new, eval_data_file, test_data_file, out_file, function=False, test_only=False, tp_indices_file=tp_indices_file), cwd=None,
						  shell=True, stdout=subprocess.PIPE) as proc:
			output = [x.decode("utf-8") for x in proc.stdout.readlines()]
			for result in output:

				if 'Effort@0.2Recall' in result:
					print(f"Effort@0.2Recall: {float(result.split(' = ')[1].rstrip())}")
					metrics[config]['Effort@0.2Recall'].append(float(result.split(' = ')[1].rstrip()))
				elif 'Recall@0.01LOC' in result:
					print(f"Recall@0.01LOC: {float(result.split(' = ')[1].rstrip())}")
					metrics[config]['Recall@0.01LOC'].append(float(result.split(' = ')[1].rstrip()))
				elif 'IFA =' in result:
					print(f"IFA: {float(result.split(' = ')[1].rstrip())}")
					metrics[config]['IFA'].append(float(result.split(' = ')[1].rstrip()))
				elif 'Top-10_Accuracy' in result:
					print(f"Top-10_Accuracy: {float(result.split(' = ')[1].rstrip())}")
					metrics[config]['Top-10_Accuracy'].append(float(result.split(' = ')[1].rstrip()))

		for metric in metrics[config]:
			if len(metrics[config][metric]) > 0:
				print(metric, np.mean(metrics[config][metric]), np.std(metrics[config][metric]))

		print('Removing final/clean model')
		os.remove(f'{model_folder}{best_model_name}')
		os.remove(train_data_file_new)


result_folder = 'results_devign/'
result_dict = {'Config': [configs[0]] * n_repeats}

for metric in ['Precision', 'Recall', 'F1', 'Effort@0.2Recall', 'Recall@0.01LOC', 'IFA', 'Top-10_Accuracy']:
	if metric in metrics[configs[0]] and len(metrics[configs[0]][metric]) > 0:
		print(metric, np.mean(metrics[config][metric]), np.std(metrics[config][metric]))
		result_dict[metric] = metrics[configs[0]][metric]
	else:
		result_dict[metric] = [-1] * n_repeats

print('Result dict:', result_dict)
results = pd.DataFrame.from_dict(result_dict)

print(f'Saving results file to {result_folder}{configs[0]}_{project_name}_{setting}_all_predict_10.csv')
results = results[['Config', 'Precision', 'Recall', 'F1', 'Effort@0.2Recall', 'Recall@0.01LOC', 'IFA', 'Top-10_Accuracy']]
print(results.values)
results.to_csv(f'{result_folder}{configs[0]}_{project_name}_{setting}_all_predict_10.csv', index=False)

print('Total execution time:', time.time() - start_time, 's.')
