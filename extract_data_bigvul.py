import os
import sys

import hashlib
from collections import Counter
import subprocess
import pickle
from linevul.TSParse import TSParse

import pandas as pd
import numpy as np
import json
import imblearn
from imblearn.under_sampling import RandomUnderSampler
from imblearn.over_sampling import RandomOverSampler

from git import Commit, Repo
from pydriller import Repository, ModificationType, Git as PyDrillerGitRepo


def generate_data(setting, szz, seed=42):

    data_folder = 'data_bigvul'
    project_name = 'bigvul'

    all_data = pd.read_parquet(f'{data_folder}/bigvul.parquet')
    
    my_vuln_df = all_data[(all_data['label'] == 1) & (all_data['latent'] == 0)]
    # my_vuln_df = my_vuln_df[(~my_vuln_df['flaw_line'].isnull()) & (my_vuln_df['flaw_line'] != '')]

    my_non_vuln_df = all_data[(all_data['label'] == 0) & (all_data['latent'] == 0)]

    v_latent_df = all_data[(all_data['label'] == 1) & (all_data['latent'] == 1) & (all_data['method'] == 'v')]

    my_vuln_df['hash_code'] = my_vuln_df['code'].map(lambda x: hashlib.sha256(str.encode(x)).hexdigest()).astype(str)
    my_non_vuln_df['hash_code'] = my_non_vuln_df['code'].map(lambda x: hashlib.sha256(str.encode(x)).hexdigest()).astype(str)

    v_latent_df['hash_code'] = v_latent_df['code'].map(lambda x: hashlib.sha256(str.encode(x)).hexdigest()).astype(str)

    my_vuln_df.drop_duplicates(subset=['hash_code'], keep='first', inplace=True)
    my_non_vuln_df.drop_duplicates(subset=['hash_code'], keep='first', inplace=True)

    v_latent_df.drop_duplicates(subset=['hash_code'], keep='first', inplace=True)

    combined_df = pd.concat([my_vuln_df, my_non_vuln_df], ignore_index=True)
    
    combined_df.drop_duplicates(subset=['hash_code'], keep='first', inplace=True)
    combined_df = combined_df.sample(frac=1, random_state=seed).reset_index(drop=True)

    split_point = 0.8

    train_df = combined_df.iloc[:int(split_point * len(combined_df))]

    rem_df = combined_df[int(split_point * len(combined_df)):]
    val_df = rem_df[:int(0.5*len(rem_df))]
    test_df = rem_df[int(0.5*len(rem_df)):]

    print('Train-val-test:', Counter(train_df['label']), Counter(val_df['label']), Counter(test_df['label']))

    if szz == 'v':
        train_latent_df = v_latent_df.copy()

    # V-SZZ
    val_df = val_df[~((val_df['label'] == 0) &
                      (val_df['hash_code'].isin(v_latent_df['hash_code'].values)))]

    val_df = val_df[~((val_df['label'] == 1) &
                      (val_df['hash_code'].isin(v_latent_df['hash_code'].values)))]

    test_df = test_df[~((test_df['label'] == 0) &
                      (test_df['hash_code'].isin(v_latent_df['hash_code'].values)))]

    test_df = test_df[~((test_df['label'] == 1) &
                      (test_df['hash_code'].isin(v_latent_df['hash_code'].values)))]

    # if use_latent:
    if 'func_id' in train_df.columns:
        train_latent_df = train_latent_df[train_latent_df['vfc'].isin(train_df['func_id'].values)]
    else:
        print('Using vfc column instead')
        train_latent_df = train_latent_df[train_latent_df['vfc'].isin(train_df['vfc'].values)]
    latent_df = pd.concat([train_latent_df, train_df], ignore_index=True)
    print('Size latent before removing duplicates:', len(latent_df), Counter(latent_df['label']))

    latent_df.drop_duplicates(subset=['hash_code'], keep='first', inplace=True)
    print('Size latent after removing duplicates:', len(latent_df), Counter(latent_df['label']))

    project_name = 'bigvul'

    print('Saving vuln, non-vuln with and without RUS and ROS')
    train_df.to_parquet(f'{data_folder}/train_regular_{project_name}_{setting}_{seed}_all.parquet', index=False)
    
    val_df.to_parquet(f'{data_folder}/val_{project_name}_{setting}_{seed}_all.parquet', index=False)
    test_df.to_parquet(f'{data_folder}/test_{project_name}_{setting}_{seed}_all.parquet', index=False)

    print('Saving latent files')
    latent_df.to_parquet(f'{data_folder}/train_latent_{szz}_{project_name}_{setting}_{seed}_all.parquet', index=False)
    
    
if __name__ == '__main__':

    n_repeats = 10
    
    for i in range(n_repeats):
        data_seed = (i + 1) * 10000
        
        for szz in ['v']:
        
            print('\n########\n')
            print(f'Generate data for {szz}-SZZ with {data_seed}')
            print('\n########\n')
            
            generate_data(setting = sys.argv[1], szz = szz, seed=data_seed)
    