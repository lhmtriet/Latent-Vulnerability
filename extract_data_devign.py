import torch

import hashlib
from collections import Counter
import subprocess
import pickle
import sys
from linevul.TSParse import TSParse

import pandas as pd
import numpy as np
import json
import imblearn
from imblearn.under_sampling import RandomUnderSampler
from imblearn.over_sampling import RandomOverSampler

from git import Commit, Repo
from pydriller import Repository, ModificationType, Git as PyDrillerGitRepo


def generate_data(setting, szz, seed=42):

	data_folder = 'data_devign'
	project_name = 'qemu'

	project_name = 'qemu'
	my_vuln_df = pd.read_parquet(f'{data_folder}/{project_name}_vuln_data_all.parquet')
	my_non_vfc_df = pd.read_parquet(f'{data_folder}/{project_name}_non_vfc_data_all.parquet')

	v_latent_df = pd.read_parquet(f'{data_folder}/my_{project_name}_latent_data_all.parquet')

	project_name = 'ffmpeg'

	my_vuln_df = pd.concat([my_vuln_df, pd.read_parquet(f'{data_folder}/{project_name}_vuln_data_all.parquet')], ignore_index=True)
	# my_vuln_df = my_vuln_df[(~my_vuln_df['flaw_line'].isnull()) & (my_vuln_df['flaw_line'] != '')]
	
	my_non_vuln_df = pd.concat([my_non_vuln_df, pd.read_parquet(f'{data_folder}/{project_name}_non_vuln_data_all.parquet')], ignore_index=True)
	my_non_vfc_df = pd.concat([my_non_vfc_df, pd.read_parquet(f'{data_folder}/{project_name}_non_vfc_data_all.parquet')], ignore_index=True)
	
	v_latent_df = pd.concat([v_latent_df, pd.read_parquet(f'{data_folder}/my_{project_name}_latent_data_all.parquet')], ignore_index=True)
	
	my_vuln_df['hash_code'] = my_vuln_df['code'].map(lambda x: hashlib.sha256(str.encode(x)).hexdigest()).astype(str)
	my_non_vfc_df['hash_code'] = my_non_vfc_df['code'].map(lambda x: hashlib.sha256(str.encode(x)).hexdigest()).astype(str)

	v_latent_df['hash_code'] = v_latent_df['code'].map(lambda x: hashlib.sha256(str.encode(x)).hexdigest()).astype(str)

	my_vuln_df.drop_duplicates(subset=['hash_code'], keep='first', inplace=True)
	my_non_vfc_df.drop_duplicates(subset=['hash_code'], keep='first', inplace=True)

	v_latent_df.drop_duplicates(subset=['hash_code'], keep='first', inplace=True)

	my_vuln_df['latent'] = 0
	my_non_vfc_df['latent'] = 0

	v_latent_df['latent'] = 1
	
	combined_df = pd.concat([my_vuln_df, my_non_vfc_df], ignore_index=True)	   

	combined_df.drop_duplicates(subset=['hash_code'], keep='first', inplace=True)
	combined_df = combined_df.sample(frac=1, random_state=seed).reset_index(drop=True)
	
	split_point = 0.8

	train_df = combined_df.iloc[:int(split_point * len(combined_df))]

	rem_df = combined_df[int(split_point * len(combined_df)):]
	val_df = rem_df[:int(0.5*len(rem_df))]
	test_df = rem_df[int(0.5*len(rem_df)):]

	print('Train-val-test:', Counter(train_df['label']), Counter(val_df['label']), Counter(test_df['label']))

	if szz == 'v':
		train_latent_df = v_latent_df.copy()

	# V-SZZ
	val_df = val_df[~((val_df['label'] == 0) &
					  (val_df['hash_code'].isin(v_latent_df['hash_code'].values)))]

	val_df = val_df[~((val_df['label'] == 1) &
					  (val_df['hash_code'].isin(v_latent_df['hash_code'].values)))]

	test_df = test_df[~((test_df['label'] == 0) &
					  (test_df['hash_code'].isin(v_latent_df['hash_code'].values)))]

	test_df = test_df[~((test_df['label'] == 1) &
					  (test_df['hash_code'].isin(v_latent_df['hash_code'].values)))]

	# if use_latent:
	if 'func_id' in train_df.columns:
		train_latent_df = train_latent_df[train_latent_df['vfc'].isin(train_df['func_id'].values)]
	else:
		print('Using vfc column instead')
		train_latent_df = train_latent_df[train_latent_df['vfc'].isin(train_df['vfc'].values)]
	latent_df = pd.concat([train_latent_df, train_df], ignore_index=True)
	print('Size latent before removing duplicates:', len(latent_df), Counter(latent_df['label']))

	latent_df.drop_duplicates(subset=['hash_code'], keep='first', inplace=True)
	print('Size latent after removing duplicates:', len(latent_df), Counter(latent_df['label']))

	project_name = 'all'

	print('Saving original vuln, non-vuln')
	train_df.to_parquet(f'{data_folder}/train_regular_{project_name}_{setting}_{seed}_all.parquet', index=False)
	
	val_df.to_parquet(f'{data_folder}/val_{project_name}_{setting}_{seed}_all.parquet', index=False)
	test_df.to_parquet(f'{data_folder}/test_{project_name}_{setting}_{seed}_all.parquet', index=False)

	print('Saving latent files')
	latent_df.to_parquet(f'{data_folder}/train_latent_{szz}_{project_name}_{setting}_{seed}_all.parquet', index=False)


if __name__ == '__main__':

	n_repeats = 10
	
	for i in range(n_repeats):
		data_seed = (i + 1) * 10000
		
		for szz in ['v']:
		
			print('\n########\n')
			print(f'Generate data for {szz}-SZZ with {data_seed}')
			print('\n########\n')
			
			generate_data(setting = sys.argv[1], szz = szz, seed=data_seed)
